---
title: "Practical Machine Learning"
author: "Malgorzata M. Jakubowska"
date: "15 September 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The aim of the following report is to provide the reader with the overview of the process of building a machine learning algorithm based on the Weight Lifting Exercise Dataset (Velloso, Bulling, Gellersen, Ugulino & Fuks, 2013). As opposed to predicting the type of activity performed, the algorithm is thought to model the "goodness" of the performance, which is a new strain of the human activity recognition studies. The report will cover the buliding procedure step by step. 
***
The Weight Lifting Exercise Dataset includes data collected from six participants performing the Unilateral Dumbbell Biceps Curl in five distinct ways. Those ways correspond to the "classe" variable, which is supposed to be predicted by other variables from the dataset. Class A is assigned to a perfect performance and Classes B-E reflect different errors. Apart from the "classe" variable both sets (training and test) contain 159 variables. There are 19622 observations in the training set and 20 in the test set.

##Data reading
The first step was to read the data into R.
```{r}
training<- read.csv("pml-training.csv")
testing<- read.csv("pml-testing.csv")
```

##Data pre-processing
Due to lack of factor variables other than "classe" no dummy variables were needed. However, zero and near zero-variance predictors as well as variables with NA values need to be identified:

```{r }
#reading necessary packages
library(caret)
#identifying near zero-variance predictors
near0<-nzv(training)
training3<- training[, -near0]
#removing variables with NA values
training4<-Filter(function(x)!any(is.na(x)), training3)
#removing first six variables (user name, timestamps, window)
training5<- training4[, c(7:59)]
```

The resulting training dataset consists of 53 variables instead of the initial 160. The next step is to further pre-process the data. Having in mind that the training set includes rather large amount of variables, principal component analysis can be applied to possibly reduce the number of variables, so the model training and tuning will be easier to perform. However, since k-fold cross validation will be used, preProcess function will not be calculated separately, but passed as an argument in the train function later, so the PCA is applied to every fold.

An important point must be made here. Due to the possibility of overloading R with excessive amount of data, the following code was used to enable usage of parallel processing.

```{r }
library(parallel)
library(doParallel)
cluster<-makeCluster(detectCores()-1)
registerDoParallel(cluster)
```

Running this chunk of code should shorten the time needed to perform the next steps of modeling. 

Next, trainControl function is used to generate parameters to later be passed to the train function. The resampling method is k-fold cross-validation with k parameter set to 10. 

```{r }
control <- trainControl(method = "cv",
                 number = 10,
                 allowParallel = TRUE)

```

Now, the initial model training can be performed with the following code. First, seed is set to ensure reproducibility. The model will be trained using random forest approach, following the article by Velloso et al. (2013) in which this procedure was also used. Additionally, as was mentioned before, preProcess function is a part of chunk above, in order to ensure that the principal component analysis is applied to every fold. 

```{r }
set.seed(1000)
fit <- train(classe~., method="rf",data=training5, preProcess=c("pca"), trControl = control)
fit
fit$finalModel

```

As can be seen in the output, all predictors were centered, scaled and used in the principal component analysis. The resampling method was 10-fold cross-validation. The final mtry parameter chosen was 2, which means that two variables were tried at each split in the classification tree. 
Accuracy of the final model is 0.98, Kappa is equal to and out-of-sample error is 1.67%. It can be concluded that the model performance is satisfactory. Below one can find a confusion matrix for the fit. 



```{r }
imp<-varImp(fit)

```

                          
The final step would be to predict the "classe" variable on the test set by inputting the following code:

```{r }
prediction<-predict(fit, testing)
prediction
```




